<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F10%2F~5ptpvlke.42k%2F</url>
    <content type="text"><![CDATA[&#31616;&#21333;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979; code{white-space: pre;} /* https://github.com/markdowncss/retro */ pre, code { font-family: Menlo, Monaco, "Courier New", monospace; } pre { padding: .5rem; line-height: 1.25; overflow-x: scroll; } @media print { *, *:before, *:after { background: transparent !important; color: #000 !important; box-shadow: none !important; text-shadow: none !important; } a, a:visited { text-decoration: underline; } a[href]:after { content: " (" attr(href) ")"; } abbr[title]:after { content: " (" attr(title) ")"; } a[href^="#"]:after, a[href^="javascript:"]:after { content: ""; } pre, blockquote { border: 1px solid #999; page-break-inside: avoid; } thead { display: table-header-group; } tr, img { page-break-inside: avoid; } img { max-width: 100% !important; } p, h2, h3 { orphans: 3; widows: 3; } h2, h3 { page-break-after: avoid; } } a, a:visited { color: #01ff70; } a:hover, a:focus, a:active { color: #2ecc40; } .retro-no-decoration { text-decoration: none; } html { font-size: 12px; } @media screen and (min-width: 32rem) and (max-width: 48rem) { html { font-size: 15px; } } @media screen and (min-width: 48rem) { html { font-size: 16px; } } body { line-height: 1.85; } p, .retro-p { font-size: 1rem; margin-bottom: 1.3rem; } h1, .retro-h1, h2, .retro-h2, h3, .retro-h3, h4, .retro-h4 { margin: 1.414rem 0 .5rem; font-weight: inherit; line-height: 1.42; } h1, .retro-h1 { margin-top: 0; font-size: 3.998rem; } h2, .retro-h2 { font-size: 2.827rem; } h3, .retro-h3 { font-size: 1.999rem; } h4, .retro-h4 { font-size: 1.414rem; } h5, .retro-h5 { font-size: 1.121rem; } h6, .retro-h6 { font-size: .88rem; } small, .retro-small { font-size: .707em; } /* https://github.com/mrmrs/fluidity */ img, canvas, iframe, video, svg, select, textarea { max-width: 100%; } html, body { background-color: #222; min-height: 100%; } html { font-size: 18px; } body { color: #fafafa; font-family: "Courier New"; line-height: 1.45; margin: 6rem auto 1rem; max-width: 48rem; padding: .25rem; } pre { background-color: #333; } blockquote { border-left: 3px solid #01ff70; padding-left: 1rem; } &#31616;&#21333;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979; 2018-06-10 17:56:28 &#20170;&#22825;&#20027;&#35201;&#23558;&#26152;&#22825;&#24471;&#21040;&#30340;&#25968;&#25454;&#65292;&#25918;&#21040;&#20043;&#21069;&#30475;&#21040;&#30340;&#27169;&#22411;&#20013;&#36305;&#20102;&#19968;&#19979;&#65292;&#30475;&#20102;&#19968;&#19979;&#25928;&#26524;&#65292;&#31616;&#21333;&#21465;&#36848;&#19968;&#19979;&#23454;&#39564;&#12290; &#23454;&#39564;&#24605;&#36335;&#65306; &#23558;&#25552;&#21462;&#30340;Scala&#21494;&#23376;&#33410;&#28857;&#30340;&#29305;&#24449;&#20316;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#36755;&#20837;&#21040;AutoenCODE&#20013; &#65306;AutoenCODE is a Deep Learning infrastructure that allows to encode source code fragments into vector representations, which can be used to learn similarities. https://github.com/micheletufano/AutoenCODE &#22522;&#26412;&#19978;&#25152;&#26377;&#30340;&#20195;&#30721;&#36825;&#20010;&#32593;&#31449;&#24050;&#32463;&#25552;&#20379;&#20102;&#65292;&#25152;&#20197;&#21482;&#35201;&#23558;&#20195;&#30721;clone&#21040;&#26412;&#22320;&#65292;&#37197;&#32622;&#19968;&#19979;&#29615;&#22659;&#23601;&#21487;&#20197;&#24320;&#22987;&#25105;&#20204;&#30340;&#23454;&#39564;&#12290;&#20855;&#20307;&#30340;AutoenCODE&#30340;&#21407;&#29702;&#65292;&#25105;&#20250;&#22312;&#20197;&#21518;&#30340;&#21338;&#23458;&#20013;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#26412;&#31687;&#21338;&#23458;&#20027;&#35201;&#35762;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#12290; &#23454;&#39564;&#27493;&#39588;&#65306; &#25353;&#29031;AutoenCODE&#32473;&#30340;&#25945;&#31243;&#65292;&#31532;&#19968;&#27493;&#26159;&#23558;&#25105;&#20204;&#25972;&#29702;&#30340;&#25968;&#25454;&#36716;&#21270;&#25104;&#35789;&#21521;&#37327;&#65292;&#36825;&#37324;&#20182;&#20351;&#29992;&#30340;&#24037;&#20855;&#26159;word2vec&#65292;&#36825;&#37324;&#27880;&#24847;&#19968;&#19979;&#65292;&#20182;&#30340;&#36825;&#20010;word2vec&#38656;&#35201;build&#20294;&#26159;windows&#31995;&#32479;&#19981;&#25903;&#25345;&#36825;&#20010;build&#65292;&#25152;&#20197;&#25105;&#23558;&#36716;&#21270;&#35789;&#21521;&#37327;&#30340;&#36825;&#37096;&#20998;&#30340;&#24037;&#20316;&#36716;&#31227;&#21040;&#20102;centos&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#65292;&#26368;&#32456;&#24471;&#21040;&#20102;&#27979;&#35797;&#26679;&#26412;&#30340;&#25152;&#26377;&#30340;&#35789;&#21521;&#37327;&#30340;&#25968;&#25454;&#12290; &#25509;&#19979;&#26469;&#23601;&#23558;&#36825;&#20010;&#35789;&#21521;&#37327;&#36755;&#20837;&#21040;Recursive Autoencoder(&#35770;&#25991;&#20013;&#25552;&#21450;&#26159;&#19968;&#20010;&#26031;&#22374;&#31119;&#30340;&#24773;&#24863;&#20998;&#26512;&#22120;)&#20013;&#21435;&#65292;&#26368;&#32456;&#24471;&#21040;&#20116;&#20010;&#32467;&#26524;&#25991;&#20214;(&#36825;&#37324;&#25552;&#21450;&#19968;&#28857;&#65292;&#35757;&#32451;&#26102;&#38388;&#23454;&#22312;&#26159;&#22826;&#38271;&#20102;&#65292;&#28304;&#20195;&#30721;&#26159;&#20351;&#29992;matlab&#20889;&#30340;&#12290;27000&#26465;&#25968;&#25454;&#25972;&#25972;&#36305;&#20102;&#19968;&#20010;&#23567;&#26102;&#65292;&#32780;&#19988;CUP&#28385;&#36127;&#33655;&#36816;&#34892;&#65292;&#21487;&#33021;&#26159;&#25105;&#30340;&#30005;&#33041;&#37197;&#32622;&#20302;&#65292;&#20043;&#21518;&#38656;&#35201;&#20248;&#21270;)&#12290;&#20998;&#21035;&#26159;&#65306; data.mat contains the input data including the corpus, vocabulary (a 1-by-|V| cell array), and We (the m-by-|V| word embedding matrix where m is the size of the word vectors). So columns of We correspond to word embeddings. corpus.dist.matrix.mat contains the distance matrix saved as matlab file. The values in the distance matrix are doubles that represent the Euclidean distance between two sentences. In particular, the cell (i,j) contains the Euclidean distance between the i-th sentence (i.e., i-th line in corpus.src) and the j-th sentence in the corpus. corpus.dist.matrix.csv contains the distance matrix saved as .csv file. corpus.sentence_codes.mat contain the embeddings for each sentence in the corpus. The sentence_codes object contains the representations for sentences, and the pairwise Euclidean distance between these representations are used to measure similarity. detector.mat contains opttheta (the trained clone detector), hparams, and options. &#36825;&#37324;&#23545;&#25105;&#20204;&#26368;&#26377;&#29992;&#30340;&#23601;&#26159;&#37027;&#20010;&#30697;&#38453;&#65292;&#23427;&#26174;&#31034;&#20004;&#21477;&#35805;&#30340;&#36317;&#31163;&#22823;&#23567;&#65292;&#36234;&#23567;&#36234;&#30456;&#20284;&#12290; &#37027;&#20040;&#22823;&#30340;&#30697;&#38453;&#65292;&#24590;&#20040;&#36827;&#34892;&#20998;&#26512;&#65281;&#65281;&#65292;&#21482;&#33021;&#30828;&#30528;&#22836;&#30382;&#36890;&#36807;&#20889;matlab&#20195;&#30721;&#65292;&#23558;&#30697;&#38453;&#20013;&#27599;&#34892;&#30340;&#26368;&#23567;&#20540;&#65288;&#38750;&#38646;&#65289;&#25552;&#21462;&#20986;&#26469;&#65292;&#36825;&#26679;&#23601;&#33021;&#24471;&#21040;27000&#22810;&#20010;&#26368;&#23567;&#20540;&#65292;&#28982;&#21518;&#20877;&#36890;&#36807;&#36825;27000&#20010;&#26368;&#23567;&#20540;&#36827;&#34892;&#31579;&#36873;&#65292;&#22240;&#20026;&#26412;&#27425;&#23454;&#39564;&#20027;&#35201;&#30475;&#19968;&#19979;&#25928;&#26524;&#25152;&#20197;&#27809;&#26377;&#27880;&#24847;&#21040;&#37027;&#20040;&#22810;&#30340;&#32454;&#33410;&#65292;&#20808;&#25226;&#26368;&#23567;&#20540;&#27714;&#20986;&#26469;&#20808;&#30475;&#30475;&#12290; &#28982;&#21518;&#25105;&#23558;&#26368;&#23567;&#20540;&#21448;&#36827;&#34892;&#20102;&#21010;&#20998;&#65292;&#35770;&#25991;&#20013;&#35828;&#20182;&#20204;&#30340;&#24819;&#27861;&#26159;&#22914;&#26524;&#36317;&#31163;&#23567;&#20110;1e-8&#23601;&#35748;&#20026;&#20182;&#20204;&#26159;&#20811;&#38534;&#30340;&#20195;&#30721;&#65292;&#28982;&#21518;&#25105;&#20197;&#36825;&#20010;&#20026;&#20998;&#30028;&#32447;&#36827;&#34892;&#20102;&#31579;&#36873;&#65292;&#21457;&#29616;&#21482;&#26377;5&#23545;&#31526;&#21512;&#35201;&#27714;&#65292;&#26368;&#32456;&#30340;&#32467;&#26524;&#22312;&#26368;&#32456;&#32467;&#26524;&#37027;&#37324;&#36827;&#34892;&#23637;&#31034;&#12290;&#27714;&#21462;&#26368;&#23567;&#20540;&#20195;&#30721;&#65306;]]></content>
  </entry>
  <entry>
    <title><![CDATA[简单代码克隆检测]]></title>
    <url>%2F2018%2F06%2F10%2F~nos3k0vs.2cu%2F</url>
    <content type="text"><![CDATA[今天主要将昨天得到的数据，放到之前看到的模型中跑了一下，看了一下效果，简单叙述一下实验。 实验思路： 将提取的Scala叶子节点的特征作为文本数据，输入到AutoenCODE中 ：AutoenCODE is a Deep Learning infrastructure that allows to encode source code fragments into vector representations, which can be used to learn similarities. https://github.com/micheletufano/AutoenCODE基本上所有的代码这个网站已经提供了，所以只要将代码clone到本地，配置一下环境就可以开始我们的实验。具体的AutoenCODE的原理，我会在以后的博客中详细的解释，本篇博客主要讲如何使用这个框架。 实验步骤： 按照AutoenCODE给的教程，第一步是将我们整理的数据转化成词向量，这里他使用的工具是word2vec，这里注意一下，他的这个word2vec需要build但是windows系统不支持这个build，所以我将转化词向量的这部分的工作转移到了centos服务器上进行，最终得到了测试样本的所有的词向量的数据。 接下来就将这个词向量输入到Recursive Autoencoder(论文中提及是一个斯坦福的情感分析器)中去，最终得到五个结果文件(这里提及一点，训练时间实在是太长了，源代码是使用matlab写的。27000条数据整整跑了一个小时，而且CUP满负荷运行，可能是我的电脑配置低，之后需要优化)。分别是： data.mat contains the input data including the corpus, vocabulary (a 1-by-|V| cell array), and We (the m-by-|V| word embedding matrix where m is the size of the word vectors). So columns of We correspond to word embeddings. corpus.dist.matrix.mat contains the distance matrix saved as matlab file. The values in the distance matrix are doubles that represent the Euclidean distance between two sentences. In particular, the cell (i,j) contains the Euclidean distance between the i-th sentence (i.e., i-th line in corpus.src) and the j-th sentence in the corpus. corpus.dist.matrix.csv contains the distance matrix saved as .csv file. corpus.sentence_codes.mat contain the embeddings for each sentence in the corpus. The sentence_codes object contains the representations for sentences, and the pairwise Euclidean distance between these representations are used to measure similarity. detector.mat contains opttheta (the trained clone detector), hparams, and options.这里对我们最有用的就是那个矩阵，它显示两句话的距离大小，越小越相似。那么大的矩阵，怎么进行分析！！，只能硬着头皮通过写matlab代码，将矩阵中每行的最小值（非零）提取出来，这样就能得到27000多个最小值，然后再通过这27000个最小值进行筛选，因为本次实验主要看一下效果所以没有注意到那么多的细节，先把最小值求出来先看看。然后我将最小值又进行了划分，论文中说他们的想法是如果距离小于1e-8就认为他们是克隆的代码，然后我以这个为分界线进行了筛选，发现只有5对符合要求，最终的结果在最终结果那里进行展示。求取最小值代码：]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>code_detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单代码克隆检测]]></title>
    <url>%2F2018%2F06%2F10%2Fexperment2%2F</url>
    <content type="text"><![CDATA[今天主要将昨天得到的数据，放到之前看到的模型中跑了一下，看了一下效果，简单叙述一下实验。 实验思路： 将提取的Scala叶子节点的特征作为文本数据，输入到AutoenCODE中 ：AutoenCODE is a Deep Learning infrastructure that allows to encode source code fragments into vector representations, which can be used to learn similarities. https://github.com/micheletufano/AutoenCODE基本上所有的代码这个网站已经提供了，所以只要将代码clone到本地，配置一下环境就可以开始我们的实验。具体的AutoenCODE的原理，我会在以后的博客中详细的解释，本篇博客主要讲如何使用这个框架。 实验步骤： 按照AutoenCODE给的教程，第一步是将我们整理的数据转化成词向量，这里他使用的工具是word2vec，这里注意一下，他的这个word2vec需要build但是windows系统不支持这个build，所以我将转化词向量的这部分的工作转移到了centos服务器上进行，最终得到了测试样本的所有的词向量的数据。 接下来就将这个词向量输入到Recursive Autoencoder(论文中提及是一个斯坦福的情感分析器)中去，最终得到五个结果文件(这里提及一点，训练时间实在是太长了，源代码是使用matlab写的。27000条数据整整跑了一个小时，而且CUP满负荷运行，可能是我的电脑配置低，之后需要优化)。分别是： data.mat contains the input data including the corpus, vocabulary (a 1-by-|V| cell array), and We (the m-by-|V| word embedding matrix where m is the size of the word vectors). So columns of We correspond to word embeddings. corpus.dist.matrix.mat contains the distance matrix saved as matlab file. The values in the distance matrix are doubles that represent the Euclidean distance between two sentences. In particular, the cell (i,j) contains the Euclidean distance between the i-th sentence (i.e., i-th line in corpus.src) and the j-th sentence in the corpus. corpus.dist.matrix.csv contains the distance matrix saved as .csv file. corpus.sentence_codes.mat contain the embeddings for each sentence in the corpus. The sentence_codes object contains the representations for sentences, and the pairwise Euclidean distance between these representations are used to measure similarity. detector.mat contains opttheta (the trained clone detector), hparams, and options. 这里对我们最有用的就是那个矩阵，它显示两句话的距离大小，越小越相似。那么大的矩阵，怎么进行分析！！，只能硬着头皮通过写matlab代码，将矩阵中每行的最小值（非零）提取出来，这样就能得到27000多个最小值，然后再通过这27000个最小值进行筛选，因为本次实验主要看一下效果所以没有注意到那么多的细节，先把最小值求出来先看看。然后我将最小值又进行了划分，论文中说他们的想法是如果距离小于1e-8就认为他们是克隆的代码，然后我以这个为分界线进行了筛选，发现只有5对符合要求，最终的结果在最终结果那里进行展示。求取最小值代码： 实验结果： 当判断距离为1e-8时(5对) 当判断距离为1e-4时(75对) 当判断距离为1e-2时(800多对)通过观察主要分为以下几个类型： 函数重载和相似函数(在同一个文件中)(5对)(800多对)(75对)(75对)D:\Git\spark\core\src\test\scala\org\apache\spark\deploy\master\MasterSuite.scalaD:\Git\spark\core\src\test\scala\org\apache\spark\deploy\master\MasterSuite.scala(75对) 父子继承关系或者同时继承同一个父类的子类之间(不同文件)D:\Git\spark\core\src\main\scala\org\apache\spark\scheduler\DAGScheduler.scala (父类)D:\Git\spark\core\src\test\scala\org\apache\spark\scheduler\TaskSetManagerSuite.scala (子类)(75对)D:\Git\spark\core\src\test\scala\org\apache\spark\scheduler\SparkListenerWithClusterSuite.scalaD:\Git\spark\core\src\test\scala\org\apache\spark\deploy\LogUrlsStandaloneSuite.scala(800对) 相似或者相同的函数(不同文件)D:\Git\spark\core\src\main\scala\org\apache\spark\util\collection\PrimitiveKeyOpenHashMap.scalaD:\Git\spark\graphx\src\main\scala\org\apache\spark\graphx\util\collection\GraphXPrimitiveKeyOpenHashMap.scala(75对)D:\Git\spark\core\src\main\scala\org\apache\spark\deploy\history\HistoryServerArguments.scalaD:\Git\spark\core\src\main\scala\org\apache\spark\deploy\worker\WorkerArguments.scala(75对) 不像是克隆的函数(我的观点)D:\Git\spark\core\src\main\scala\org\apache\spark\status\LiveEntity.scalaD:\Git\spark\core\src\main\scala\org\apache\spark\status\LiveEntity.scala(75对) 实验总结： 由于时间和人手有限，现在只是对这几个结果进行了分析，还有很多对都没有看，之后找时间看看还有没有其他类型，或者老师可以分配几个人帮我看看。 附录： 在得到结果以后，这是忘了如何去找源文件，这里我在原来的parse的基础上加上了一个统计样本所在的文件的文件，通过行数来查找对应的文件，感觉很费时费力。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>code_detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AST树提取进展]]></title>
    <url>%2F2018%2F06%2F09%2Fexperment1%2F</url>
    <content type="text"><![CDATA[今天主要把昨天没有做完的工作进行了扩展，由提取单一文件的叶子节点扩展到提取到整个项目的叶子节点，然后将类级别的数据修改成方法级别的数据 实验步骤： 按照昨天写的代码，只需要加一个循环遍历文件的函数就可以了 然后第一个问题出现了，scala meta 这个工具还不是很成熟，对于部分文件在parse的时候会抛出异常 在网上查了好久，网上也有许多人遇到这个问题，但是scala meta并没有提供解决的办法。经过不懈的努力，最终在评论区找到了解决方法，这个bug主要是s&quot;xxxxxxx&quot;后面直接换行引起的(黑人问号)，只要在\n后面加一个空格就可以了(黑人问号)。 所以我就对我们输入的数据进行了预处理，所有包含字符串s&quot;xxxxx&quot;的行的\n都进行了变换。 第二个问题，嵌套函数的问题，因为我们测试的数据是在方法级别上的进行抽取，所以就会出现嵌套函数的问题，具体的嵌套函数的示例如下图所示： 在这里的问题主要是将子函数抽取出来作为一条数据还是将子函数作为一条像if语句那样的句子作为父函数的一部分，经过跟几位老师讨论，我们决定采用第一种方式，原因是第一子函数的粒度小，第二就是在函数的功能上面还是子函数为主，对于第四种类型的代码克隆的判断来说更加有利。所以我利用栈的思想将子函数剥离出来：最终的结果是： 其中13是父函数，12是子函数。 最终结果： 成功提取spark代码里面的27042条样本，明天开始进行测试算法效果，并且再读一遍论文，整理一下文档加油！！！！ 还要好好学一下英语和线代和算法！！！]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>AST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AST]]></title>
    <url>%2F2018%2F06%2F08%2FAST%2F</url>
    <content type="text"><![CDATA[Scala AST 叶子节点提取 背景： 前几天由于考试没有来得及整理基础知识，今天在这里先整理一下这两天做的Scala叶子节点的值提取 Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。Scala 运行在Java虚拟机上，并兼容现有的Java程序。Scala 源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库。(百度百科） 实验步骤： 使用的工具： scala meta : https://scalameta.org/ Intellij Idea: https://www.jetbrains.com/idea/ 借助的参考资料： scala meta的示例程序 浏览器 构造 scala AST 树：https://astexplorer.net/#/gist/22cf8a3fcb2155c087ae94b4d194c1b6/d10c646ecfae4c69c919408aa3aaefb2deda2df7 实验带代码： 查看 scala meta 源程序可以发现 ，该工具里面有一个Tree的类，该类有children 属性和parent属性： 所以可以根据这个类来进行遍历得到我们需要的叶子节点的数据，在这里我采用visitor的方式来进行遍历。主要的遍历的对象有以下几个： 分别代表Scala中的各个语法，这里在做的时候出现了几个问题。一个是Term.param和Type.param 需要“精准的查找”，不能像其他的Term.Name,Term.Annonate那样，可以通过Term来进行查找： 也就是说其他的Term里面的属性可以通过遍历Term然后再进行查找，但是这个Term.param必须在第一次遍历的时候就指出来，难道Term.param不属于Term?很奇怪。以后再查一查。 代码的逻辑并不难，下面就开始打印叶子节点，通过观察浏览器AST解析器https://astexplorer.net/#/gist/22cf8a3fcb2155c087ae94b4d194c1b6/d10c646ecfae4c69c919408aa3aaefb2deda2df7发现：叶子节点主要在以下几个地方打印：基本数据类型Term.Name处Type.Name 处还有一个是Name处 实验的最终结果： Scala 源代码 提取的叶子节点 源代码 提取的叶子节点 总结：实验结果还未仔细观察，具体的细节有待改进，还有就是上次说的将string值改成等基本数据的转化还未加入。 本博客持续更新。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>AST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日记]]></title>
    <url>%2F2018%2F06%2F06%2Fdaily1%2F</url>
    <content type="text"><![CDATA[Please enter the password to read. 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]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件工程复习提纲]]></title>
    <url>%2F2018%2F06%2F01%2Fsoftware-review%2F</url>
    <content type="text"><![CDATA[本博客是根据软件工程最后一节重点课总结的内容，对软件工程的知识进行了简单的梳理 软件工程简述 软件工程定义： 软件工程是研究应用如何以系统性的、规范化的、可定量的过程化方法 去开发和维护软件， 以及如何把经过时间考验而证明正确的管理技术和当前能够得到的最好的技术结合起来]]></content>
      <categories>
        <category>学科复习</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
      </tags>
  </entry>
</search>
